# -*- coding: utf-8 -*-
"""Copy of CUSEDatathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dw5HCgPlKgAxlNhqtT0ICcd1WksBgUE_
"""

# Install TensorFlow (if not pre-installed)
!pip install tensorflow pandas scikit-learn seaborn matplotlib

# Import libraries
import geopandas as gpd
import pandas as pd
from shapely.geometry import Point

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
import seaborn as sns
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()  # Upload CSV files

# Load datasets
parcels = pd.read_csv('parcel_map.csv')
violations = pd.read_csv('parking_violations.csv')
cityline = pd.read_csv('cityline_requests.csv')

# Quick exploration
parcels.info()
violations.info()
cityline.info()

parcels.fillna(0, inplace=True)
violations.fillna(0, inplace=True)
cityline.fillna(0, inplace=True)

parcels_gdf = gpd.GeoDataFrame(
    parcels, geometry=gpd.points_from_xy(parcels.LONG, parcels.LAT), crs='EPSG:4326')

# Convert violations to GeoDataFrame
violations_gdf = gpd.GeoDataFrame(
    violations, geometry=gpd.points_from_xy(violations.LONG, violations.LAT), crs='EPSG:4326')

# Convert cityline to GeoDataFrame
cityline_gdf = gpd.GeoDataFrame(
    cityline, geometry=gpd.points_from_xy(cityline.Lng, cityline.Lat), crs='EPSG:4326')

# Spatial join: Parking Violations to Parcels
violations_joined = gpd.sjoin(violations_gdf, parcels_gdf, how='inner', predicate='within')
violation_counts = violations_joined.groupby('PRINTKEY').size().reset_index(name='violation_count')

# Spatial join: Cityline Requests to Parcels
cityline_joined = gpd.sjoin(cityline_gdf, parcels_gdf, how='inner', predicate='within')
request_counts = cityline_joined.groupby('PRINTKEY').size().reset_index(name='request_count')

# Merge counts back with parcels
merged = parcels.merge(violation_counts, on='PRINTKEY', how='left').merge(
    request_counts, on='PRINTKEY', how='left').fillna(0)

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Select features and target
X = merged[['violation_count', 'land_av', 'total_av', 'ACRES',]]
y = merged['request_count']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build TensorFlow model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train model
model.fit(X_train, y_train, epochs=10, validation_split=0.2)

loss, mae = model.evaluate(X_test, y_test)
print(f"Test Mean Absolute Error: {mae}")

!pip install folium

import folium
from folium.plugins import HeatMap

# Create a base map
m = folium.Map(location=[merged.LAT.mean(), merged.LONG.mean()], zoom_start=12)

# Add parcels colored by predicted request counts
for _, row in merged.iterrows():
    folium.CircleMarker(
        location=[row['LAT'], row['LONG']],
        radius=5,
        popup=f"Requests: {row['request_count']}, Violations: {row['violation_count']}",
        color='blue' if row['request_count'] < 5 else 'red',
        fill=True,
        fill_opacity=0.7
    ).add_to(m)

m.save('parcel_requests_map.html')
m

heat_data = [[row['LAT'], row['LONG'], row['request_count']] for index, row in merged.iterrows()]

heat_map = folium.Map(location=[merged.LAT.mean(), merged.LONG.mean()], zoom_start=12)
HeatMap(heat_data, radius=15).add_to(heat_map)

heat_map.save('cityline_heatmap.html')
heat_map

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(merged[['violation_count', 'request_count', 'land_av', 'total_av', 'ACRES']])
plt.show()

# Assuming 'X' is your feature set and 'model' is your trained model
merged['predicted_requests'] = model.predict(X).flatten()

# Heatmap based on predictions
heat_data_pred = [[row['LAT'], row['LONG'], row['predicted_requests']] for index, row in merged.iterrows()]

heat_map_pred = folium.Map(location=[merged.LAT.mean(), merged.LONG.mean()], zoom_start=12)
HeatMap(heat_data_pred, radius=15).add_to(heat_map_pred)

heat_map_pred.save('predicted_cityline_heatmap.html')
heat_map_pred

from google.colab import files
files.download('predicted_cityline_heatmap.html')

import numpy as np

y_pred = model.predict(X_test).flatten()

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Requests')
plt.ylabel('Predicted Requests')
plt.title('Actual vs Predicted Cityline Requests')
plt.show()

neigh_requests = merged.groupby('NHOOD')['request_count'].sum().reset_index()

# Merge with parcel GeoDataFrame for mapping
parcels_gdf = parcels_gdf.merge(neigh_requests, on='NHOOD')

# Create choropleth map
folium_map = folium.Map(location=[merged.LAT.mean(), merged.LONG.mean()], zoom_start=12)
folium.Choropleth(
    geo_data=parcels_gdf.__geo_interface__,
    data=neigh_requests,
    columns=['NHOOD', 'request_count'],
    key_on='feature.properties.NHOOD',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    legend_name='Total Cityline Requests by Neighborhood'
).add_to(folium_map)

folium_map.save('neighborhood_choropleth.html')
folium_map

# Heatmap based on actual cityline request data
heat_data_actual = [[row['LAT'], row['LONG'], row['request_count']] for index, row in merged.iterrows()]

heat_map_actual = folium.Map(location=[merged.LAT.mean(), merged.LONG.mean()], zoom_start=12)
HeatMap(heat_data_actual, radius=15).add_to(heat_map_actual)

heat_map_actual.save('actual_cityline_heatmap.html')
from google.colab import files
files.download('actual_cityline_heatmap.html')
heat_map_actual

import folium
from folium.plugins import HeatMap
import numpy as np

# Prepare actual heatmap data
heat_data_actual = [[row['LAT'], row['LONG'], row['request_count']] for index, row in merged.iterrows()]

# Example normalization (scale between 0 and 1)
max_request = max([row[2] for row in heat_data_actual])

# Prevent ZeroDivisionError
if max_request == 0:
    # Assign equal weight if all requests are zero
    heat_data_actual_normalized = [[lat, lon, 0] for lat, lon, req in heat_data_actual]
else:
    heat_data_actual_normalized = [[lat, lon, req / max_request] for lat, lon, req in heat_data_actual]

# Initialize map centered around Syracuse
m = folium.Map(location=[43.0481, -76.1474], zoom_start=12)

# Add HeatMap with updated parameters
HeatMap(
    data=heat_data_actual_normalized,
    radius=40,
    blur=30,
    min_opacity=0.5,
    max_zoom=10
).add_to(m)

# Save and download the updated map
m.save('actual_cityline_heatmap.html')

#from google.colab import files
#files.download('actual_cityline_heatmap.html')

# Display map in notebook
m